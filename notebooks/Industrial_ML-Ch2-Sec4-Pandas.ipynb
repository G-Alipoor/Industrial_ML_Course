{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNc8u9UYukkaMhon9ZV+YgX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Section 4 â€” Data Manipulations with Pandas\n","\n","Pandas is one of the most important Python libraries for **data manipulation and analysis**.  \n","It is especially useful in **machine learning workflows**, where raw data must first be **cleaned, transformed, and structured** before being used to train models.  \n","\n","You can think of Pandas as **Excel for Python â€” but far more powerful and flexible**.  \n","Where NumPy focuses on numerical arrays, Pandas focuses on **tabular data**: rows and columns.\n","\n","In this section, we will:\n","- Learn the basics of Pandas.\n","- Explore and filter data.\n","- Practice with small, **industry-inspired datasets**."],"metadata":{"id":"EtrWgDz0NJ75"}},{"cell_type":"markdown","source":["## 4.1 Series and DataFrames\n","\n","The two core Pandas objects are:\n","\n","- **Series**: a one-dimensional labeled array (like one column of a table).  \n","- **DataFrame**: a two-dimensional table with labeled rows and columns.\n","\n","Weâ€™ll start by creating Series, then build DataFrames, and finally explore ways to inspect and understand them."],"metadata":{"id":"YPywUjmlOWE1"}},{"cell_type":"code","source":[" # ---- Imports ----\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","# Let's also check the versions\n","print(\"Pandas version:\", pd.__version__)\n","print(\"Numpy version:\", np.__version__)"],"metadata":{"id":"gmDYtMptgv6I","executionInfo":{"status":"ok","timestamp":1758010495228,"user_tz":-210,"elapsed":10,"user":{"displayName":"Qasim ElÃ®pÃ»r","userId":"05803539671053130359"}},"outputId":"14db4f76-c6b8-464a-ecc9-39a8ee089afa","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["Pandas version: 2.2.2\n","Numpy version: 2.0.2\n"]}]},{"cell_type":"code","source":["# ---- Dataset Source Configuration ----\n","# Purpose: Make notebook cross-platform (Colab or local)\n","# and allow datasets to be persistent on Google Drive.\n","\n","# Detect environment\n","try:\n","    import google.colab\n","    ON_COLAB = True\n","except ImportError:\n","    ON_COLAB = False\n","\n","# Set paths based on environment and source (âš ï¸ Update COURSE_PATH below if your folder has a different name or location)\n","if ON_COLAB:\n","    # Mount Google Drive if using Colab\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","\n","    # Base folder for this course on Google Drive\n","    COURSE_PATH = \"/content/drive/MyDrive/Industrial_ML_Course\"\n","else:\n","    # Offline / local computer\n","    # Adjust COURSE_PATH to your local folder\n","    COURSE_PATH = r\"D:\\Industrial_ML_Course\"\n","\n","# Subfolders\n","DATASET_PATH = os.path.join(COURSE_PATH, \"datasets/CWRU\")\n","NOTEBOOK_PATH = os.path.join(COURSE_PATH, \"notebooks\")\n","\n","# Ensure directories exist\n","os.makedirs(DATASET_PATH, exist_ok=True)\n","os.makedirs(NOTEBOOK_PATH, exist_ok=True)\n","\n","print(\"Environment:\", \"Colab\" if ON_COLAB else \"Local\")\n","print(\"Course Path:\", COURSE_PATH)\n","print(\"Dataset Path:\", DATASET_PATH)"],"metadata":{"id":"zK6HNGBZcn-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Creating a Series from a list ----\n","\n","s = pd.Series([100, 200, 300], index=[\"a\", \"b\", \"c\"])\n","print(s, \"\\n\")\n","\n","# Accessing values\n","print(\"Value at index b:\", s[\"b\"], \"\\n\")\n","\n","# Setting dtype during creation\n","s_float = pd.Series([1, 2, 3], dtype=\"float\")\n","print(s_float)"],"metadata":{"id":"Q9G2EB4BHQOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Creating DataFrame from a dictionary ----\n","\n","df_dict = pd.DataFrame({\n","    \"Date\": pd.date_range(\"2025-10-01\", periods=3, freq=\"D\"),\n","    \"Energy_kWh\": [210, 222, 198],\n","    \"Temperature_C\": [-2, 0, 4]\n","})\n","df_dict"],"metadata":{"id":"bewfJKudHXFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Creating DataFrame from a NumPy array ----\n","\n","data = np.array([[210, -2], [222, 0], [198, 4]])\n","df_array = pd.DataFrame(\n","    data,\n","    index=[\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n","    columns=[\"Energy_kWh\", \"Temperature_C\"]\n",")\n","df_array"],"metadata":{"id":"fB17gEACHf-j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Displaying Created Dataframe ----\n","\n","from IPython.display import display\n","\n","# Overview\n","print(\"========== head ==========\")\n","display(df_dict.head())      # first rows\n","print(\"\\n========== info ==========\")\n","display(df_dict.info())               # prints to stdout\n","print(\"\\n========== describe ==========\")\n","display(df_dict.describe())  # statistics\n","print(\"\\n========== dtypes ==========\")\n","display(df_dict.dtypes)      # column types"],"metadata":{"id":"u_RirYaQHz2A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.2 Selecting and Filtering Data\n","Data selection is one of the most common Pandas tasks.  \n","With small datasets, you can quickly see and pick values.  \n","But in **large industrial datasets** (thousands of rows, multiple column types),  \n","we need systematic ways to select and filter data.\n","\n","Common operations include:\n","- Selecting **columns** (numerical, categorical, or text).  \n","- Selecting **rows** by label (`.loc`) or position (`.iloc`).  \n","- Filtering with **conditions** (e.g., energy > 280).  \n","- Combining multiple conditions (e.g., energy > 280 AND status = \"Running\").  \n","- Querying based on **categorical/string columns** (e.g., only Machine M2 operated by Sara).  \n","- Slicing rows using labels (e.g., date ranges) or integer indices.  \n","\n","These tools are essential for preparing subsets of data before analysis or machine learning."],"metadata":{"id":"inqc9B-_Ok3b"}},{"cell_type":"code","source":["# ---- Column & Row selection ----\n","\n","# Column selection\n","display(df_dict[\"Energy_kWh\"])\n","\n","# Row by label\n","display(df_dict.loc[0])\n","\n","# Row by integer position\n","display(df_dict.iloc[0])"],"metadata":{"id":"-gfx0iEhIRyr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Changing the dtaframe indexes ----\n","\n","df_dict.index = [\"x\", \"y\", \"z\"]\n","display(df_dict)\n","print(\"\\n========== Column x ==========\")\n","display(df_dict.loc[\"x\"])"],"metadata":{"id":"9U8PFHIgIzqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Selecting multiple columns ----\n","df_dict[[\"Date\", \"Temperature_C\"]]"],"metadata":{"id":"oslyrruiIfv6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Slicing rows ----\n","df_dict[0:2]\n","df_dict.iloc[::2]  # every other rows"],"metadata":{"id":"u2bDflMOIVEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Boolean Selection ----\n","\n","# Boolean masks\n","df_dict[df_dict[\"Energy_kWh\"] > 210]\n","\n","# Multiple conditions\n","df_dict[(df_dict[\"Energy_kWh\"] > 210) & (df_dict[\"Temperature_C\"] < 2)]"],"metadata":{"id":"m1nAQY5eIS53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Using query for readability ----\n","\n","df_dict.query(\"Energy_kWh > 210 and Temperature_C < 2\")"],"metadata":{"id":"qztYui5fImKH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Label ranges (with datetime index) ----\n","\n","df_idx = df_dict.set_index(\"Date\")\n","df_idx.loc[\"2025-10-01\":\"2025-10-02\"]"],"metadata":{"id":"3tdhgmDaIpK2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Larger DataFrame with mixed data types ----\n","\n","np.random.seed(42)  # reproducibility\n","\n","n_rows = 2000\n","big_df = pd.DataFrame({\n","    \"Date\": pd.date_range(\"2023-01-01\", periods=n_rows, freq=\"H\"),\n","    \"Energy_kWh\": np.random.randint(150, 300, size=n_rows),        # numerical\n","    \"Temperature_C\": np.random.normal(20, 5, size=n_rows),         # numerical\n","    \"Machine\": np.random.choice([\"Motor 1\", \"Motor 2\", \"Fan\", \"Generator\", \"Gear\"], size=n_rows),  # categorical\n","    \"Region\": np.random.choice([\" Region_A \", \"region_B\", \" REGION_C\", \"Region_A\", \"REGION_B  \"], size=n_rows),  # string\n","    \"Status\": np.random.choice([\"Running\", \"Idle\", \"Fault\"], size=n_rows),  # categorical\n","    \"Operator\": np.random.choice([\"Radmehr\", \"Sara\", \"Rojin\", \"Farzad\"], size=n_rows),  # string\n","})\n","\n","big_df.head()"],"metadata":{"id":"TKpKr7bUNEVy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Inspecting the big DataFrame ----\n","\n","print(\"Shape:\", big_df.shape, \"\\n\")\n","print(\"Columns:\", big_df.columns.tolist(), \"\\n\")\n","big_df.info()"],"metadata":{"id":"MkCbwvXrN6eq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Quick look at categorical values ----\n","print(big_df[\"Machine\"].value_counts(), \"\\n\")\n","print(big_df[\"Status\"].value_counts())"],"metadata":{"id":"dw7k9cOCN9lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Example filtering on multiple types ----\n","\n","subset = big_df[\n","    (big_df[\"Energy_kWh\"] > 280) &\n","    (big_df[\"Status\"] == \"Running\") &\n","    (big_df[\"Operator\"] == \"Sara\")\n","]\n","print(\"Subset size:\", subset.shape, \"\\n\")\n","subset.head()"],"metadata":{"id":"54YoW_LlOFck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ðŸ’¡ **Industrial Example**:  \n","Suppose we want to detect **peak demand days** in our power grid.  \n","We can filter all rows where daily consumption exceeds a threshold."],"metadata":{"id":"7Mf9YDzfO_FR"}},{"cell_type":"markdown","source":["## 4.3 Handling Missing and Duplicate Data\n","\n","Industrial datasets often contain:\n","- **Missing values** â†’ e.g., a sensor temporarily stopped recording.  \n","- **Duplicate rows** â†’ e.g., the same event logged twice in a SCADA system.  \n","\n","Before applying machine learning, we need to **detect, clean, and handle** these issues.  \n","In this subsection, we will use a **modified copy of the original dataset** to simulate such problems."],"metadata":{"id":"w0U0zvNePR8r"}},{"cell_type":"code","source":["# ---- Create a working copy of the original big DataFrame ----\n","# We will introduce missing values and duplicates in this copy\n","\n","df_modified = big_df.copy()\n","\n","# Introduce missing values in several columns\n","df_modified.loc[::50, \"Energy_kWh\"] = np.nan          # simulate sensor gaps\n","df_modified.loc[1::70, \"Temperature_C\"] = np.nan      # simulate sensor gaps\n","df_modified.loc[2::90, \"Operator\"] = None             # missing operator logs\n","\n","# Introduce duplicate rows to simulate repeated logs\n","df_modified = pd.concat([df_modified, df_modified.iloc[500:510]], ignore_index=True)\n","\n","df_modified.head(12)"],"metadata":{"id":"bOm0u76uPUZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Count missing values per column ----\n","df_modified.isnull().sum()"],"metadata":{"id":"tjSg_8nugkzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Percentage of missing data ----\n","(df_modified.isnull().mean() * 100).round(2)"],"metadata":{"id":"xk2OTKQ4gk5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Option 1: Drop rows with any missing values (may discard many rows) ----\n","\n","df_dropped = df_modified.dropna()\n","print(\"After dropping rows:\", df_dropped.shape)"],"metadata":{"id":"yVXwOD4Xgqb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Option 2: Fill missing values ----\n","\n","df_filled = df_modified.copy()\n","\n","# Fill numeric column with mean\n","df_filled[\"Temperature_C\"].fillna(df_filled[\"Temperature_C\"].mean(), inplace=True)\n","\n","# Fill categorical/string columns with placeholders\n","df_filled[\"Operator\"].fillna(\"Unknown\", inplace=True)\n","\n","df_filled.head(12)"],"metadata":{"id":"AU1MYFQLgu3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Option 3: Forward-fill (repeat last valid value, useful for time-series data) ----\n","df_ffill = df_modified.fillna(method=\"ffill\")\n","df_ffill.head(12)"],"metadata":{"id":"Qt8fH1O-gvXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Count duplicate rows ----\n","df_modified.duplicated().sum()"],"metadata":{"id":"DpHT-ibVgyNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Remove duplicates ----\n","df_no_duplicates = df_modified.drop_duplicates()\n","print(\"Original size:\", df_modified.shape, \"| After removing duplicates:\", df_no_duplicates.shape)"],"metadata":{"id":"dTprPurYg12b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ðŸ’¡ **Industrial Examples**\n","- Missing **temperature readings** can be filled with the column mean or forward-fill to maintain continuity for modeling.  \n","- Missing **operator logs** can use a placeholder like `\"Unknown\"` to avoid losing entire rows.  \n","- Repeated **fault events** should be removed to prevent bias in fault frequency analysis.  "],"metadata":{"id":"BrROusQuPl7o"}},{"cell_type":"markdown","source":["## 4.4 Data Transformation\n","\n","Data transformation is a key step before applying machine learning models.  \n","It ensures that data is in the correct format, clean, and enriched with meaningful features.\n","\n","We will cover four main types of transformations:\n","\n","1. **Renaming & Data Types** â€“ making column names and data formats consistent.  \n","2. **Feature Engineering** â€“ creating new features that better represent the data.  \n","3. **String & Text Processing** â€“ handling textual or categorical information.  \n","4. **Column-wise Operations & Scaling** â€“ applying functions, normalization, and preparing data for ML."],"metadata":{"id":"yAPxfKf7P4jf"}},{"cell_type":"markdown","source":["### 4.4.1 Column & String Transformations\n","\n","Data often needs basic cleanup at both the column level and the text level.  \n","This includes:\n","- Renaming columns for clarity  \n","- Converting between data types (e.g., strings â†’ datetime, floats â†’ integers)  \n","- Handle datetime objects for industrial data (e.g., converting timestamps, calculating durations).\n","- Cleaning string/text values (removing spaces, fixing case, replacing substrings)  \n"],"metadata":{"id":"DWgDRr3lP7j2"}},{"cell_type":"code","source":["# ---- Rename columns ----\n","df_trans = big_df.copy()\n","df_trans = df_trans.rename(columns={\"Energy_kWh\": \"Energy\",\n","                                    \"Temperature_C\": \"Temperature\"})\n","\n","print(\"Renamed columns:\\n\", df_trans.columns)"],"metadata":{"id":"X0Orimz7QbMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Convert dtypes ----\n","\n","# Example: ensure 'Machine' is string, 'Temperature' is float\n","df_trans[\"Machine\"] = df_trans[\"Machine\"].astype(str)\n","df_trans[\"Temperature\"] = df_trans[\"Temperature\"].astype(float)\n","\n","print(\"\\nColumn types after conversion:\\n\", df_trans.dtypes)"],"metadata":{"id":"fgQ9Xx2pQcx_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- String/text cleaning ---\n","df_trans[\"Region\"] = df_trans[\"Region\"].str.strip().str.upper().str.replace(\"region_\", \"r\")\n","display(df_trans)"],"metadata":{"id":"OGOw8HmXkTzt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.4.2 Feature Engineering\n","\n","- Add new columns derived from existing ones.  \n","- Example: calculate the **daily deviation from mean energy consumption**.  \n","- Encode categorical variables into numerical form so they can be used in ML models."],"metadata":{"id":"yXLkuU8lQAbh"}},{"cell_type":"code","source":["# ---- Example 1: Deviation From the Mean ----\n","\n","# Ensure datetime type\n","df_trans[\"Date\"] = pd.to_datetime(df_trans[\"Date\"])\n","\n","# Extract just the calendar date\n","df_trans[\"Day\"] = df_trans[\"Date\"].dt.date\n","\n","# Compute daily mean per calendar day\n","daily_mean = df_trans.groupby(\"Day\")[\"Energy\"].transform(\"mean\")\n","\n","# Add new feature: deviation from that mean\n","df_trans[\"Deviation\"] = (\n","    df_trans[\"Energy\"] - daily_mean\n",")\n","\n","display(df_trans.head())"],"metadata":{"id":"9M24w1etTkYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Example 2: Encode categorical variable 'Status' into numeric ----\n","status_map = {\"Running\": 1, \"Idle\": 0, \"Fault\": -1}\n","df_trans[\"Status_Code\"] = df_trans[\"Status\"].map(status_map)\n","display(df_trans.head())"],"metadata":{"id":"k6iIfNUZTmU1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.4.2 Column-wise Operations & Scaling\n","\n","- Apply functions across columns or rows using `.apply()` and vectorized operations.  \n","- Normalize or scale numeric values (e.g., standardization before ML).  \n","- Ensure numerical stability and comparability across features."],"metadata":{"id":"SBtTPu4dQE3B"}},{"cell_type":"code","source":["# ---- Example: Normalization ----\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","# Select numeric columns\n","numeric_cols = [\"Energy\", \"Temperature\"]\n","\n","# Min-Max Scaling to [0, 1]\n","scaler_minmax = MinMaxScaler()\n","df_trans[[col + \"_MinMax\" for col in numeric_cols]] = scaler_minmax.fit_transform(df_trans[numeric_cols])\n","\n","# Standardization (zero mean, unit variance)\n","scaler_std = StandardScaler()\n","df_trans[[col + \"_Std\" for col in numeric_cols]] = scaler_std.fit_transform(df_trans[numeric_cols])\n","\n","display(df_trans.head())"],"metadata":{"id":"CTsM2ulyP5Yf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.5 Grouping & Aggregation\n","\n","In real-world industrial datasets, we often need to **summarize data**:\n","- How much energy did each machine consume per day?\n","- What was the average temperature in each region?\n","- How many faults occurred per status type?\n","\n","Pandas provides powerful tools for these operations that generally fall into four main categories:\n","\n","| **Category**            | **Function(s)**                                                                 | **Description**                                                                 |\n","|--------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------|\n","| **Aggregation**          | `sum`, `mean`, `median`, `min`, `max`, `count`, `size`, `std`, `var`, `first`, `last`, `nunique`, `prod` | Reduce each group to a single scalar summary value.                             |\n","| **Transformation**       | `transform`, `rank`, `cumcount`, `cumsum`, `cumprod`                           | Return the same shape as the original data, but with values transformed per group. |\n","| **Filtering**            | `filter`                                                                       | Drop entire groups based on a condition (keep only groups satisfying a criterion). |\n","| **Flexible Apply / Custom** | `apply`, `agg` (or `aggregate`)                                              | Apply custom functions, or multiple aggregation functions across different columns. |\n","\n","âœ… **Rule of thumb**  \n","- Use **aggregation** for summaries.  \n","- Use **transform** when you need the same shape back.  \n","- Use **filter** to drop groups.  \n","- Use **apply/agg** for flexible or custom operations.\n","\n","These tools are essential for preparing features and insights in Machine Learning pipelines."],"metadata":{"id":"etvn0g2MdRwE"}},{"cell_type":"code","source":["# ---- Reuse df_trans from earlier ----\n","\n","df_group = df_trans.copy()"],"metadata":{"id":"0MlxRl4HdeJg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Group by machine and calculate total energy consumption ---\n","energy_by_machine = df_group.groupby(\"Machine\")[\"Energy\"].sum()\n","print(\"Total energy consumption per machine:\\n\", energy_by_machine.head(), \"\\n\")"],"metadata":{"id":"r5wfRa6LdScy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Group by region and calculate average temperature ---\n","avg_temp_by_region = df_group.groupby(\"Region\")[\"Temperature\"].mean()\n","print(\"Average temperature per region:\\n\", avg_temp_by_region, \"\\n\")"],"metadata":{"id":"CG7VeiNVdXGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Group by date (daily) and count number of faults ---\n","faults_per_day = df_group.groupby(df_group[\"Day\"])[\"Status_Code\"].apply(lambda x: (x == 1).sum())\n","print(\"Number of faults per day:\\n\", faults_per_day.head(), \"\\n\")"],"metadata":{"id":"0OZg80MPdgFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Multiple aggregations with .agg() ---\n","machine_summary = df_group.groupby(\"Machine\").agg({\n","    \"Energy\": [\"mean\", \"sum\", \"max\"],\n","    \"Temperature\": \"mean\"\n","})\n","print(\"Machine summary with multiple aggregations:\\n\", machine_summary.head(), \"\\n\")"],"metadata":{"id":"yL72D_VLdhMH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Group-level transformation (add column for machine-level mean energy) ---\n","df_group[\"Machine_Mean_Energy\"] = df_group.groupby(\"Machine\")[\"Energy\"].transform(\"mean\")\n","df_group[[\"Machine\", \"Energy\", \"Machine_Mean_Energy\"]].head(10)"],"metadata":{"id":"g9v2NHVwdilG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.6 Concatenation & Merging\n","\n","In real-world datasets, information often comes from multiple sources:  \n","- **Measurements** (e.g., sensor readings)  \n","- **Metadata** (e.g., machine type, location)  \n","- **External logs** (e.g., maintenance, failures)  \n","\n","To analyze such data effectively, we need to **combine DataFrames**. Pandas provides three main approaches:  \n","\n","- **Concatenation (`pd.concat`)**  \n","  - Stack DataFrames vertically (rows) or horizontally (columns).  \n","  - Useful when data has the same schema but split across files or batches.  \n","\n","- **Merge (`pd.merge`)**  \n","  - SQL-style joins on one or more keys (`inner`, `left`, `right`, `outer`).  \n","  - Flexible way to combine related information across tables.  \n","\n","These techniques allow building a **unified dataset** for deeper analysis.  "],"metadata":{"id":"TGuM97zIlJu4"}},{"cell_type":"code","source":["# ---- Example 1: Concatenation (stacking DataFrames) ----\n","\n","# Two small DataFrames with same columns\n","df_part1 = pd.DataFrame({\n","    \"Machine\": [\"Motor 1\", \"Motor 2\"],\n","    \"Energy\": [120, 150]\n","})\n","\n","df_part2 = pd.DataFrame({\n","    \"Machine\": [\"Fan\", \"Generator\"],\n","    \"Energy\": [90, 300]\n","})\n","\n","# Concatenate vertically (row-wise)\n","df_concat = pd.concat([df_part1, df_part2], ignore_index=True)\n","print(df_concat)"],"metadata":{"id":"HOmkPQdunzet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Example 2: Merge (SQL-style join) ----\n","\n","# Left DataFrame: machine info\n","df_machines = pd.DataFrame({\n","    \"Machine\": [\"Motor 1\", \"Motor 2\", \"Fan\", \"Generator\"],\n","    \"Region\": [\"North\", \"East\", \"West\", \"South\"]\n","})\n","\n","# Right DataFrame: maintenance logs\n","df_maintenance = pd.DataFrame({\n","    \"Machine\": [\"Motor 1\", \"Motor 2\", \"Gear\"],\n","    \"Last_Repair\": [\"2024-07-12\", \"2024-07-15\", \"2024-08-01\"]\n","})\n","\n","# Merge on 'Machine'\n","df_merged = pd.merge(df_machines, df_maintenance, on=\"Machine\", how=\"outer\")\n","print(df_merged)"],"metadata":{"id":"tqdq-I_Pn3kz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.7 Sorting & Ranking\n","\n","In industrial datasets, it is often useful to **sort** or **rank** observations:\n","\n","- Sort by numeric or categorical columns to identify top/bottom values.  \n","- Rank data within groups to find relative positions (e.g., top energy-consuming machines per day).  \n","\n","Pandas provides:\n","- **`sort_values()`** â†’ sort rows by column(s).  \n","- **`sort_index()`** â†’ sort rows or columns by index.  \n","- **`rank()`** â†’ assign ranks within a Series or DataFrame, with options for handling ties.  \n","\n","These operations are useful for **feature engineering**, exploratory analysis, and reporting."],"metadata":{"id":"clTls-k5o6Go"}},{"cell_type":"code","source":["# ---- Copy the transformed DataFrame ----\n","\n","df_sort = df_trans.copy()"],"metadata":{"id":"jHB7q_Rro9ZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Sort by single column ---\n","\n","df_sorted_energy = df_sort.sort_values(\"Energy\", ascending=False)\n","print(\"Top 5 energy-consuming rows:\\n\", df_sorted_energy[[\"Day\", \"Energy\", \"Temperature\", \"Machine\", \"Region\", \"Status\"]].head())"],"metadata":{"id":"VCsNmWFfo-10"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Sort by multiple columns (lexicographic: first by Region, then by Temperature) ---\n","\n","df_sorted_multi = df_sort.sort_values([\"Region\", \"Temperature\"], ascending=[True, False])\n","print(\"Sorted by Region and Temperature:\\n\", df_sorted_energy[[\"Day\", \"Energy\", \"Temperature\", \"Machine\", \"Region\", \"Status\"]].head(), \"\\n\")"],"metadata":{"id":"PcIo4vcapChM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Rank within a column ---\n","\n","# ascending=False â†’ highest value gets rank 1;\n","# method=\"min\" â†’ ties share the lowest possible rank\n","df_sort[\"Energy_Rank\"] = df_sort[\"Energy\"].rank(method=\"min\", ascending=False).astype(int)\n","df_sort[[\"Machine\", \"Energy\", \"Energy_Rank\"]].head(10)"],"metadata":{"id":"tIbSBRbwpDi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Rank within groups ---\n","\n","df_sort[\"Daily_Energy_Rank\"] = df_sort.groupby(df_sort[\"Date\"].dt.date)[\"Energy\"].rank(ascending=False, method=\"dense\").astype(int)\n","df_sort[[\"Date\", \"Machine\", \"Energy\", \"Daily_Energy_Rank\"]].head(10)"],"metadata":{"id":"enRKR8OJpEbS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.8 Loading & Saving Data\n","\n","In industry projects, data often comes from files such as CSV, Excel, or MATLAB `.mat`.  \n","In this subsection, we show how to **read** these files into pandas DataFrames and how to **save** DataFrames back to files for later use.\n","\n","We'll use examples based on the `big_df` structure we've created earlier, so you can experiment immediately without needing external files."],"metadata":{"id":"ZU6HDplkwTQM"}},{"cell_type":"code","source":["# --- Save a DataFrame to CSV and Excel ---\n","\n","# Save to CSV\n","big_df.to_csv(os.path.join(DATASET_PATH, \"big_df_example.csv\"), index=False)\n","\n","# Save to Excel\n","big_df.to_excel(os.path.join(DATASET_PATH, \"big_df_example.xlsx\"), sheet_name=\"Data\", index=False)\n","\n","print(\"Saved big_df to CSV and Excel successfully!\")"],"metadata":{"id":"u2YiJJN_xcmj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Read CSV and Excel back ---\n","\n","# Read CSV\n","df_csv = pd.read_csv(os.path.join(DATASET_PATH, \"big_df_example.csv\"))\n","print(\"CSV loaded:\")\n","print(df_csv.head())\n","\n","# Read Excel\n","df_excel = pd.read_excel(os.path.join(DATASET_PATH, \"big_df_example.xlsx\"), sheet_name=\"Data\")\n","print(\"\\nExcel loaded:\")\n","print(df_excel.head())"],"metadata":{"id":"uLMwzHgPxrqN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Saving and reading MATLAB .mat files ---\n","\n","import scipy.io\n","\n","# Convert DataFrame columns to numpy arrays\n","data_dict = {col: big_df[col].to_numpy() for col in big_df.columns}\n","\n","# Save to .mat file\n","scipy.io.savemat(os.path.join(DATASET_PATH, \"big_df_example.mat\"), mdict=data_dict)\n","print(\"Saved big_df to MATLAB .mat file!\")\n","\n","# Load .mat file\n","mat_data = scipy.io.loadmat(os.path.join(DATASET_PATH, \"big_df_example.mat\"))\n","\n","# Convert loaded data back to DataFrame\n","df_mat = pd.DataFrame({k: v.flatten() for k, v in mat_data.items() if not k.startswith(\"__\")})\n","print(\"\\nLoaded .mat data as DataFrame:\")\n","print(df_mat.head())"],"metadata":{"id":"YWfrwyyh0I9w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Final Exercises â€” Practice with Pandas\n","\n","Use the `df_trans` DataFrame created in this notebook (our industrial dataset) to solve the tasks below.  \n","These exercises cover concepts from Sections 4.1â€“4.7 and will help consolidate your learning.\n","\n","---\n","\n","#### 1. Basic Exploration\n","- Display the first 10 rows and check column data types.\n","- Check for missing values in all columns.\n","- Compute basic statistics (mean, min, max) for numeric columns.\n","\n","#### 2. Column Operations\n","- Rename `'Energy'` â†’ `'Energy_kWh'`.\n","- Create a new column `'Temp_F'` converting `'Temperature'` to Fahrenheit.\n","- Create a flag column `'High_Energy'` for rows where `'Energy_kWh' > 400`.\n","\n","#### 3. Handling Missing Data\n","- Introduce some NaNs in `'Temperature'` and `'Energy_kWh'` (e.g., every 60th row).\n","- Fill NaNs in `'Temperature'` with the column mean.\n","- Drop rows where `'Energy_kWh'` is NaN.\n","\n","#### 4. String & Text Processing\n","- Clean the `'Region'` column (strip spaces, lowercase, replace `'region_'` with `'Region'`).\n","- Create a new column `'Machine_Type'` based on `'Machine'` (e.g., extract `'Motor'`, `'Fan'`, etc.).\n","\n","#### 5. Grouping & Aggregation\n","- Compute total and average energy per `'Machine'`.\n","- Count the number of faults per `'Region'`.\n","- Compute standard deviation and variance of `'Temperature'` per `'Region'`.\n","- Rank machines by average daily energy consumption.\n","- Filter groups to keep only regions with more than 100 samples.\n","\n","#### 6. Concatenation & Merging\n","- Create a small DataFrame with last maintenance dates for some machines.\n","- Merge it with `df_trans` on `'Machine'` using a left join.\n","- Concatenate a subset of `df_trans` (e.g., first 5 rows) to itself vertically.\n","\n","#### 7. Sorting & Ranking\n","- Sort `df_trans` by `'Energy_kWh'` descending.\n","- Sort `df_trans` by `'Region'` and then `'Temperature'` ascending.\n","- Create a new column `'Energy_Rank_Global'` ranking all rows by `'Energy_kWh'`.\n","- Create a new column `'Energy_Rank_Daily'` ranking rows within each day.\n","\n","#### 8. Loading & Saving Data\n","- Try modifying `big_df` (e.g., adding missing values, new columns) and save it again.\n","- Compare the differences in types when reading/writing CSV, Excel, and `.mat`.\n","- Combine this with the data cleaning and transformation tools learned in previous subsections.\n","\n","#### 9. Optional: Your Choice Function\n","- Apply one additional GroupBy function of your choice (e.g., `nunique`, `cumsum`, `first`) to explore.\n","- Explain in a comment why you chose this function.\n","\n","---\n","\n","ðŸ’¡ **Hints:**  \n","Use pandas methods such as:\n","- `head()`, `info()`, `describe()`, `dtypes`  \n","- `rename()`, `astype()`, `apply()`, `str.strip()`, `str.lower()`, `str.replace()`  \n","- `isnull()`, `fillna()`, `dropna()`  \n","- `groupby()`, `agg()`, `transform()`, `filter()`, `rank()`  \n","- `merge()`, `join()`, `concat()`  \n","- `sort_values()`, `sort_index()`"],"metadata":{"id":"xWgutgiYMhPd"}},{"cell_type":"markdown","source":["Next â†’ **Section 5: Data Visualization**"],"metadata":{"id":"FeJ8lpNxf1Ys"}}]}