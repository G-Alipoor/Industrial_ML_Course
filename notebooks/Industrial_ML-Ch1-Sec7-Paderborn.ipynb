{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiUCsloR3yiLemNjcLm30G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 📓 Course Notebook — From Python Basics to Industrial Data Preprocessing\n","\n","Welcome! This notebook is designed to **evolve step by step** with the course.  \n","Instead of starting fresh each time, we will **gradually extend this notebook** as new topics are introduced.\n","\n","### 🔹 Current Scope\n","- **Chapter 2 — Python Basics & Exploratory Signal Analysis**  \n","  Learn Python essentials for scientific computing, plotting, and working with real industrial datasets.  \n","  Explore signals in the **time domain, frequency domain, and using rolling statistics**.  \n","  Automate feature extraction and visualization.  \n","\n","- **Chapter 3 — Data Preprocessing**  \n","  Prepare raw signals for machine learning.  \n","  Cover techniques such as **handling missing values, outlier detection, noise filtering, scaling, transformations, and feature extraction**.  \n","  Practice splitting data into training, validation, and test subsets.\n","\n","### 🔹 Upcoming Chapters\n","- **Chapter 4 — Classical Machine Learning**  \n","  Build and evaluate models using **supervised and unsupervised ML techniques** (e.g., classification, clustering, dimensionality reduction).  \n","  Apply cross-validation, feature selection, and model comparison.  \n","\n","- **Chapter 5 — Deep Learning for Industrial Data**  \n","  Design and train **neural networks** for time-series and signal data.  \n","  Explore architectures such as CNNs, RNNs, and Transformers, and compare them with classical ML methods.\n","\n","### 🔹 Your Parallel Project\n","Alongside the Paderborn bearing dataset used in this notebook, each student must choose **one industrial dataset** at the beginning of the course.  \n","At the end of each chapter, apply the learned concepts and preprocessing steps on your **own dataset**, ensuring it is ready for the next chapter.  \n","This step-by-step extension will become your **final course project**.\n","\n","### 🔹 Exercises\n","- Each chapter ends with a **dedicated exercise section**.  \n","- Exercises are practical and progressive — you are expected to **complete all exercises** at the end of each chapter.  \n","- Some tasks are marked as *optional* for deeper exploration.  \n","\n","---\n","\n","✅ By the end of Chapter 3, you should be comfortable with:  \n","- Using Python for industrial data analysis.  \n","- Exploring, cleaning, and transforming raw signals.  \n","- Preparing datasets (Paderborn + your own) for **feature extraction and machine learning** in the next chapter."],"metadata":{"id":"8EKVtiU2p5NW"}},{"cell_type":"markdown","source":["# 📘 **Chapter 2: Python for Machine Learning**\n","---\n","\n","This chapter introduces Python tools for working with industrial datasets.  \n","Skills here (data loading, visualization, statistics) are foundations for later chapters on ML (Ch. 4) and DL (Ch. 5).  \n","\n","**Datasets used:** Paderborn (main example)."],"metadata":{"id":"IZ1_j_djyQgJ"}},{"cell_type":"code","source":["# ---- Imports ----\n","\n","import os\n","import numpy as np\n","import scipy.io as sio\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import urllib.request\n","from urllib.parse import urljoin\n","import ssl\n","import shutil\n","import platform\n","import subprocess\n","\n","# Ensure inline plotting\n","%matplotlib inline\n","\n","try:\n","    import rarfile\n","except ImportError:\n","    !pip install rarfile\n","    import rarfile"],"metadata":{"id":"8SKFKoUaq7MM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Dataset Source Configuration ----\n","\n","# Detect environment\n","try:\n","    import google.colab\n","    ON_COLAB = True\n","except ImportError:\n","    ON_COLAB = False\n","\n","if ON_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    COURSE_PATH = \"/content/drive/MyDrive/Industrial_ML_Course\"\n","else:\n","    COURSE_PATH = r\"D:\\Industrial_ML_Course\"  # adjust if needed\n","\n","DATASET_PATH = os.path.join(COURSE_PATH, \"datasets/Paderborn\")\n","NOTEBOOK_PATH = os.path.join(COURSE_PATH, \"notebooks\")\n","\n","os.makedirs(DATASET_PATH, exist_ok=True)\n","os.makedirs(NOTEBOOK_PATH, exist_ok=True)\n","\n","print(\"Environment:\", \"Colab\" if ON_COLAB else \"Local\")\n","print(\"Dataset Path:\", DATASET_PATH)\n","print(\"Notebook Path:\", NOTEBOOK_PATH)"],"metadata":{"id":"bVjkrU77q_Dh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Download and Extract Dataset ----\n","\n","sample_file = 'K001.rar'  # example file\n","dataset_url = 'https://groups.uni-paderborn.de/kat/BearingDataCenter/'  # official hosting\n","file_url = urljoin(dataset_url, sample_file)\n","file_path = os.path.join(DATASET_PATH, sample_file)\n","extracted_folder = os.path.splitext(file_path)[0]\n","\n","ssl_context = ssl._create_unverified_context()\n","\n","if not os.path.exists(extracted_folder):\n","    if not os.path.exists(file_path):\n","        print(f\"Downloading {sample_file} ...\")\n","        with urllib.request.urlopen(file_url, context=ssl_context) as response, open(file_path, 'wb') as out_file:\n","            shutil.copyfileobj(response, out_file)\n","        print(\"Download completed!\")\n","    else:\n","        print(f\"{sample_file} already exists.\")\n","\n","    print(f\"Extracting {sample_file} ...\")\n","    try:\n","        with rarfile.RarFile(file_path) as rf:\n","            rf.extractall(DATASET_PATH)\n","    except rarfile.Error:\n","        print(\"rarfile failed, trying WinRAR...\")\n","        winrar_path = r\"C:\\Program Files\\WinRAR\\WinRAR.exe\"  # adjust if needed\n","        subprocess.run([winrar_path, \"x\", \"-o+\", file_path, DATASET_PATH], check=True)\n","    print(\"Extraction completed!\")\n","\n","    # Optional cleanup (commented out for inspection)\n","    # if os.path.exists(file_path):\n","    #     os.remove(file_path)\n","    #     print(f\"{sample_file} removed after extraction.\")\n","else:\n","    print(f\"Extracted folder already exists: {extracted_folder}\")\n","\n","# List all .mat files\n","mat_files = []\n","for root, dirs, files in os.walk(extracted_folder):\n","    for f in files:\n","        if f.endswith(\".mat\"):\n","            mat_files.append(os.path.join(root, f))\n","\n","print(f\"Found {len(mat_files)} .mat files. Examples:\", mat_files[:5])"],"metadata":{"id":"9I-7lTbCrBrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Load a specific .mat file ----\n","\n","mat_file = mat_files[0]  # first file\n","print(\"Loading:\", mat_file)\n","\n","data = sio.loadmat(mat_file, squeeze_me=True, struct_as_record=False)\n","\n","keys = [k for k in data.keys() if not k.startswith('__')]\n","print(\"Measurement keys:\", keys)\n","\n","measurement = data[keys[0]]  # usually one measurement per file\n","print(\"Available fields:\", measurement._fieldnames)"],"metadata":{"id":"9rp2HeZNrQhA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Access 'Info' field ----\n","\n","info = measurement.Info\n","if hasattr(info, '__dict__'):\n","    print(\"Info fields:\", vars(info).keys())\n","    for k, v in vars(info).items():\n","        print(f\"{k}: {type(v)} -> {v}\")\n","else:\n","    print(\"Info:\", info)"],"metadata":{"id":"WsgxQ9SFrRIR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Access Y (sensor recordings) ----\n","\n","Y = measurement.Y\n","\n","print(\"Available signals:\")\n","if isinstance(Y, np.ndarray):\n","    for i, sig in enumerate(Y.flat):\n","        fs = getattr(sig, \"Raster\", \"N/A\")\n","        shape = getattr(sig, \"Data\", np.array([])).shape\n","        length = len(sig.Data) if hasattr(sig, \"Data\") else 0\n","        print(f\"{i+1:2d}. {sig.Name:25s} | fs={fs:<10} | shape={shape} | length={length}\")\n","else:\n","    fs = getattr(Y, \"Raster\", \"N/A\")\n","    shape = getattr(Y, \"Data\", np.array([])).shape\n","    length = len(Y.Data) if hasattr(Y, \"Data\") else 0\n","    print(f\" 1. {Y.Name:25s} | fs={fs:<10} | shape={shape} | length={length}\")"],"metadata":{"id":"1ec-FUGmrZW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Select and visualize a signal ----\n","\n","signal_index = 1\n","max_samples_to_plot = 5000\n","\n","sig = Y.flat[signal_index] if isinstance(Y, np.ndarray) else Y\n","fs = sig.Raster if hasattr(sig, \"Raster\") and isinstance(sig.Raster, (int,float)) else 64000\n","data_to_plot = sig.Data[:max_samples_to_plot]\n","\n","print(f\"Selected Signal: {sig.Name}\")\n","print(f\"Sampling frequency: {fs} Hz\")\n","print(f\"Data shape: {sig.Data.shape}\")\n","\n","# Time-domain plot\n","plt.figure(figsize=(12,4))\n","plt.plot(data_to_plot)\n","plt.title(f\"Signal {signal_index+1}: {sig.Name} (first {len(data_to_plot)} samples)\")\n","plt.xlabel(\"Sample index\")\n","plt.ylabel(\"Amplitude\")\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"tsyFDF-praIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Rolling Statistics ----\n","\n","signal_series = pd.Series(data_to_plot)\n","rolling_mean = signal_series.rolling(window=200).mean()\n","rolling_std = signal_series.rolling(window=200).std()\n","rolling_rms = signal_series.rolling(window=200).apply(lambda x: np.sqrt(np.mean(x**2)))\n","\n","print(\"Rolling mean (first 5):\", rolling_mean.dropna().head().values)\n","\n","plt.figure(figsize=(12,4))\n","plt.plot(signal_series, label=\"Original\", alpha=0.7)\n","plt.plot(rolling_mean, label=\"Rolling Mean\", linewidth=2)\n","plt.plot(rolling_std, label=\"Rolling Std\", linewidth=2)\n","plt.plot(rolling_rms, label=\"Rolling RMS\", linewidth=2)\n","plt.title(f\"Rolling Statistics - {sig.Name}\")\n","plt.xlabel(\"Sample index\")\n","plt.ylabel(\"Amplitude\")\n","plt.legend()\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"oA5juUcZrcUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Frequency-domain analysis ----\n","\n","from scipy.fft import fft, fftfreq\n","\n","N = len(data_to_plot)\n","T = 1/fs\n","yf = fft(data_to_plot)\n","xf = fftfreq(N, T)[:N//2]\n","\n","plt.figure(figsize=(12,4))\n","plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n","plt.title(f\"Frequency Spectrum - {sig.Name}\")\n","plt.xlabel(\"Frequency [Hz]\")\n","plt.ylabel(\"Amplitude\")\n","plt.xlim(0, fs/2)\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"TbnRNQk6zEcL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Spectrogram (Time-Frequency) ----\n","\n","from scipy.signal import spectrogram\n","\n","f, t, Sxx = spectrogram(data_to_plot, fs=fs)\n","plt.figure(figsize=(12,4))\n","plt.pcolormesh(t, f, 10*np.log10(Sxx), shading='gouraud')\n","plt.title(f\"Spectrogram - {sig.Name}\")\n","plt.ylabel(\"Frequency [Hz]\")\n","plt.xlabel(\"Time [s]\")\n","plt.colorbar(label=\"Power [dB]\")\n","plt.ylim(0, fs/2)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"5eaDdpVNzILw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Optional: Batch load all signals ----\n","\n","def load_all_signals(mat_files):\n","    signals = []\n","    for mat_file in mat_files:\n","        data = sio.loadmat(mat_file, squeeze_me=True, struct_as_record=False)\n","        measurement_key = [k for k in data.keys() if not k.startswith('__')][0]\n","        measurement = data[measurement_key]\n","        Y = measurement.Y\n","        if isinstance(Y, np.ndarray):\n","            for sig in Y.flat:\n","                signals.append(sig.Data)\n","        else:\n","            signals.append(Y.Data)\n","    return signals\n","\n","all_signals = load_all_signals(mat_files)\n","print(f\"Total signals loaded: {len(all_signals)}\")"],"metadata":{"id":"S1px6dkCzwbe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📝 Exercises — Chapter 2: Python Basics\n","\n","These exercises are meant to reinforce **Python programming, plotting, and exploratory data handling**. You will work both on the Paderborn dataset and your own chosen industrial dataset (your “project dataset”).\n","\n","1. **Signal Comparison**\n","   - Load two signals from different conditions (healthy vs faulty).\n","   - Plot them in:\n","     - **Time domain**\n","     - **Frequency domain (FFT)**\n","     - **Rolling statistics** (mean, RMS, std)\n","   - Observe differences in amplitude and patterns.\n","   - Take into account that different signals may have **different sampling frequencies or lengths**.\n","\n","2. **Rolling Statistics**\n","   - Experiment with different **window sizes** for rolling mean, RMS, or standard deviation.\n","   - How does smoothing affect visibility of patterns or trends?\n","\n","3. **Spectrogram Analysis**\n","   - Generate **spectrograms** for multiple signals.\n","   - Compare energy distributions for healthy vs faulty conditions.\n","   - Identify patterns that may indicate faults.\n","\n","4. **Automation Challenge**\n","   - Write a function to **load multiple `.mat` files** and extract a **feature matrix**.\n","   - Features to extract (time-domain):\n","     - mean, std, RMS, peak-to-peak, skewness, kurtosis\n","   - If signals are long, divide them into **non-overlapping frames/windows** before feature extraction.\n","   - Visualize relationships between extracted features using scatter plots or heatmaps.\n","   - Do certain features form **distinct clusters** for healthy vs faulty bearings?  \n","     *Hint:* If features cluster differently, this suggests they can help distinguish healthy vs faulty conditions. You can **optionally** use dimensionality reduction (PCA, t-SNE) to visualize clustering in 2D/3D.\n","\n","5. **Project Dataset Exploration**\n","   - Repeat all experiments and explorations from this notebook (plotting, FFT, rolling statistics, etc.) on your chosen industrial dataset.\n","   - Make sure your dataset is **ready for preprocessing in Chapter 3**."],"metadata":{"id":"lyRvQwI575er"}},{"cell_type":"markdown","source":["# 📘 **Chapter 3: Data Preprocessing**\n","---\n","\n","This chapter extends our Paderborn dataset exploration to cover:\n","\n","1. **Data Cleaning**  \n","   - Removing Duplicates  \n","   - Handling Missing Values  \n","   - Outlier Detection & Correction  \n","   - Noise Filtering  \n","\n","2. **Data Transformation**  \n","   - Scaling (Normalization & Standardization)  \n","   - Distribution Transforms (Power, Log)  \n","   - Categorical Encoding (Label & One-hot)  \n","\n","3. **Feature Extraction**  \n","\n","4. **Data Splitting & Validation**  "],"metadata":{"id":"7I38eC5wydrH"}},{"cell_type":"code","source":["# ---- Duplicate Detection (Meaningful for Time-Amplitude Pairs) ----\n","'''\n","⚡ Key Point:\n","For raw 1D signals, repeated amplitude values are natural (e.g., sine waves)\n","and do NOT indicate duplication.\n","\n","In time-series data, however, a *duplicate sample* means the same\n","(time, amplitude) pair is logged more than once — usually due to\n","logging or communication errors. These duplicates should be removed.\n","'''\n","\n","# Represent the signal as a time–amplitude DataFrame (so we can check duplicates properly)\n","time_index = np.arange(len(signal_series))\n","signal_df = pd.DataFrame({\"time\": time_index, \"amplitude\": signal_series})\n","\n","# ✅ Simulate duplicates by repeating the first 200 rows\n","duplicated_df = pd.concat([signal_df, signal_df.iloc[:200]], ignore_index=True)\n","\n","print(\"Original length:\", len(signal_df))\n","print(\"With duplicates:\", len(duplicated_df))\n","print(\"Duplicates present?\", duplicated_df.duplicated().any())\n","\n","# Show a few duplicated rows for illustration\n","print(\"\\nExample duplicated samples:\")\n","print(duplicated_df[duplicated_df.duplicated()].head())\n","\n","# Remove duplicates (drops rows where *both* time and amplitude are repeated)\n","clean_df = duplicated_df.drop_duplicates()\n","print(\"\\nAfter removing duplicates:\", len(clean_df))\n","print(\"Number of rows removed:\", len(duplicated_df) - len(clean_df))"],"metadata":{"id":"jcqlNhWWznOO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Adding Missing Values (Simulated) ---\n","'''\n","The Paderborn dataset we are using is clean — there are no missing samples.\n","However, in real-world industrial systems, sensors sometimes fail to record data points\n","due to communication errors, noise, or temporary malfunctions.\n","To practice, let’s **simulate missing values** by randomly dropping some samples,\n","and then apply the techniques you learned earlier to handle them.\n","'''\n","\n","# Convert signal into a pandas Series (easier to manipulate)\n","signal_series = pd.Series(data_to_plot.copy())\n","\n","# Introduce missing values (simulate 10% of samples as NaN)\n","rng = np.random.default_rng(seed=42)\n","missing_indices = rng.choice(signal_series.index, size=int(0.1 * len(signal_series)), replace=False)\n","signal_series.loc[missing_indices] = np.nan\n","\n","print(\"Number of missing values:\", signal_series.isna().sum())\n","\n","# Plot signal with missing values\n","plt.figure(figsize=(12, 3))\n","plt.subplot(2,1,1)\n","plt.plot(signal_series, color='red', alpha=0.7)\n","plt.title(\"Signal with Simulated Missing Values\")\n","plt.ylabel(\"Amplitude\")\n","plt.subplot(2,1,2) # Zoom in to better visualize missing gaps\n","plt.plot(signal_series[:500], color='red', alpha=0.7)\n","plt.title(\"Zoomed-In Signal with Simulated Missing Values\")\n","plt.xlabel(\"Sample index\")\n","plt.ylabel(\"Amplitude\")\n","plt.show()"],"metadata":{"id":"ifasqIJR9a-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Handling Missing Values ----\n","'''\n","Strategies to Handle Missing Values in Time Series:\n","\n","1. Drop missing values – risky: creates irregular gaps in time, breaking continuity.\n","2. Forward/Backward fill – simple; assumes stability of signal over gaps.\n","3. Interpolation – often preferred; estimates values smoothly between neighbors.\n","\n","👉 In industrial signals, interpolation is usually more realistic than filling\n","with constant values or dropping data.\n","'''\n","\n","# Forward fill\n","ffill_signal = signal_series.fillna(method=\"ffill\")\n","\n","# Backward fill\n","bfill_signal = signal_series.fillna(method=\"bfill\")\n","\n","# Linear interpolation\n","interp_signal = signal_series.interpolate(method=\"linear\")\n","\n","# Plot comparison\n","plt.figure(figsize=(12, 4))\n","plt.subplot(211)\n","plt.plot(signal_series, label=\"Original (with NaN)\", color=\"red\", alpha=0.5)\n","plt.plot(interp_signal, label=\"Interpolated\", color=\"blue\")\n","plt.legend()\n","plt.title(\"Handling Missing Values in the Signal\")\n","plt.xlabel(\"Sample index\")\n","plt.ylabel(\"Amplitude\")\n","plt.subplot(212) # Zoom in to better visualize missing gaps\n","plt.plot(signal_series[:500], label=\"Original (with NaN)\", color=\"red\", alpha=0.5)\n","plt.plot(interp_signal[:500], label=\"Interpolated\", color=\"blue\")\n","plt.legend()\n","plt.title(\"Handling Missing Values in the Signal (Zoomed-In)\")\n","plt.xlabel(\"Sample index\")\n","plt.ylabel(\"Amplitude\")\n","plt.show()"],"metadata":{"id":"Tn1CqDIs-G-9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["⚠️ **Note on Handling Missing Values in Signals**\n","\n","- Industrial signals are often continuous, so **missing samples** usually come from sensor faults, communication errors, or data loss.  \n","- Simply **dropping NaNs** can break the time series and distort analysis.  \n","- Better strategies:  \n","  - **Forward/Backward fill**: copies last/next valid sample (good for short gaps).  \n","  - **Interpolation**: estimates values smoothly between neighbors (better for gradual changes).  \n","- The choice depends on the **context**: for vibration signals, interpolation often works well; for status signals (on/off), forward fill may be more realistic.  "],"metadata":{"id":"urMqWsheKNN0"}},{"cell_type":"code","source":["# ---- Outlier Detection (Z-score method) ----\n","\n","signal_series = pd.Series(data_to_plot)\n","\n","z_scores = (signal_series - signal_series.mean()) / signal_series.std()\n","outliers = np.where(np.abs(z_scores) > 3)[0]\n","print(f\"Detected {len(outliers)} outliers\")\n","\n","plt.figure(figsize=(12,3))\n","plt.plot(signal_series, label=\"Signal\")\n","plt.scatter(outliers, signal_series.iloc[outliers], color='red', label=\"Outliers\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"rP5bjJy_zTJf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["⚠️ **Note on Outlier Detection in Signals**\n","\n","- Outliers are samples that deviate strongly from the normal signal range.  \n","- Common sources in industrial data:  \n","  - Sensor glitches or random noise spikes  \n","  - Short-term machine/system anomalies  \n","- The **Z-score method** marks values far from the mean (e.g., > 3 standard deviations).  \n","- Interpretation is key:  \n","  - Outliers may be **true faults** or just **measurement errors**.  \n","  - Decide whether to remove, correct, or keep them using domain knowledge.  \n","- In 1D signals, small isolated spikes are common — don’t misclassify normal variations as outliers.  "],"metadata":{"id":"wak7Qod77Ccr"}},{"cell_type":"code","source":["# ---- Noise Filtering (Median Filter) ----\n","'''\n","Median filtering is a simple and robust method to reduce short-term spikes/noise\n","while preserving the overall shape of the signal.\n","\n","- Why median? Unlike mean filters, it is less sensitive to outliers and sharp spikes.\n","- Here we apply it with kernel_size=5 (uses 5-sample sliding window).\n","- We fill NaNs beforehand (forward-fill) to avoid issues during filtering.\n","'''\n","\n","from scipy.signal import medfilt\n","\n","# Fill NaNs before filtering\n","signal_filled = signal_series.fillna(method=\"ffill\")\n","\n","# Apply median filter\n","filtered_signal = medfilt(signal_filled, kernel_size=5)\n","\n","# Plot comparison\n","plt.figure(figsize=(12,3))\n","plt.plot(signal_filled, alpha=0.6, label=\"Original (filled)\")\n","plt.plot(filtered_signal, label=\"Median Filtered\", color=\"orange\")\n","plt.legend()\n","plt.title(\"Noise Filtering with Median Filter\")\n","plt.show()"],"metadata":{"id":"_ZqRTqOLzVdw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["⚠️ **Note on Noise Filtering in Signals**\n","\n","- Industrial signals often contain **high-frequency noise** from sensors or electrical interference.  \n","- **Median filtering** reduces noise while **preserving edges and sharp transitions**.  \n","- How it works:  \n","  - Each sample is replaced by the **median** of neighboring values within a window.  \n","  - Especially effective for removing **impulse spikes** or isolated noise points.  \n","- Interpretation:  \n","  - Smoothed signals reveal underlying trends more clearly.  \n","  - Window size matters: too large → removes meaningful variations; too small → noise remains.  "],"metadata":{"id":"_HQm8w78LKFv"}},{"cell_type":"code","source":["# ---- Scaling (Normalization & Standardization) ----\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","\n","# Convert to numpy 2D (required by sklearn)\n","X = signal_series.fillna(method=\"ffill\").values.reshape(-1, 1)\n","\n","# Normalization (Min-Max)\n","scaler_minmax = MinMaxScaler()\n","X_minmax = scaler_minmax.fit_transform(X)\n","\n","# Standardization (Z-score)\n","scaler_std = StandardScaler()\n","X_std = scaler_std.fit_transform(X)\n","\n","# Plot histograms\n","plt.figure(figsize=(12, 4))\n","plt.hist(X, bins=50, alpha=0.5, label=\"Original\", density=True)\n","plt.hist(X_minmax, bins=50, alpha=0.5, label=\"Min-Max\", density=True)\n","plt.hist(X_std, bins=50, alpha=0.5, label=\"Standardized\", density=True)\n","plt.legend()\n","plt.title(\"Distribution After Scaling (Normalization & Standardization)\")\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Density\")\n","plt.show()"],"metadata":{"id":"x-nM7OtSkcdd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Distribution Transforms (Power & Log) ----\n","\n","from sklearn.preprocessing import PowerTransformer\n","\n","# Convert to numpy 2D (required by sklearn)\n","X = signal_series.fillna(method=\"ffill\").values.reshape(-1, 1)\n","\n","# Power transform (Yeo-Johnson, handles zeros/negatives)\n","pt = PowerTransformer(method='yeo-johnson')\n","X_pt = pt.fit_transform(X)\n","\n","# Plot histograms\n","plt.figure(figsize=(12, 4))\n","plt.hist(X, bins=50, alpha=0.5, label=\"Original\", density=True)\n","plt.hist(X_pt, bins=50, alpha=0.5, label=\"Power Transform (Yeo-Johnson)\", density=True)\n","plt.legend()\n","plt.title(\"Distribution After Distribution Transforms\")\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Density\")\n","plt.show()"],"metadata":{"id":"3SQU6Cb6kvcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- Scaling & Transformations ----\n","\n","from sklearn.preprocessing import PowerTransformer\n","\n","# Convert to numpy 2D (required by sklearn)\n","X = signal_series.fillna(method=\"ffill\").values.reshape(-1, 1)\n","\n","# Normalization (Min-Max)\n","scaler_minmax = MinMaxScaler()\n","X_minmax = scaler_minmax.fit_transform(X)\n","\n","# Standardization (Z-score)\n","scaler_std = StandardScaler()\n","X_std = scaler_std.fit_transform(X)\n","\n","# Power transform (Yeo-Johnson, handles zeros/negatives)\n","pt = PowerTransformer(method='yeo-johnson')\n","X_pt = pt.fit_transform(X)\n","\n","# Plot histograms\n","plt.figure(figsize=(12, 4))\n","plt.hist(X, bins=50, alpha=0.5, label=\"Original\", density=True)\n","plt.hist(X_minmax, bins=50, alpha=0.5, label=\"Min-Max\", density=True)\n","plt.hist(X_std, bins=50, alpha=0.5, label=\"Standardized\", density=True)\n","plt.hist(X_pt, bins=50, alpha=0.5, label=\"Power Transform\", density=True)\n","plt.legend()\n","plt.title(\"Distribution After Different Transformations\")\n","plt.xlabel(\"Value\")\n","plt.ylabel(\"Density\")\n","plt.show()"],"metadata":{"id":"wiW6tfiPzXRA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["⚠️ **Notes on Scaling & Transformations**\n","\n","**🔹 What we are doing**\n","- Scaling and transformations make features more suitable for machine learning.  \n","- In this example, we compare:  \n","  - **Original signal distribution** (nearly sinusoidal current signal).  \n","  - **Min–Max scaling** → squeezes values into `[0, 1]`.  \n","  - **Standardization (z-score)** → mean = 0, variance = 1.  \n","  - **Power Transform (Yeo–Johnson)** → reduces skewness and makes distributions closer to Gaussian.  \n","\n","\n","\n","**🔹 Observations from the plots**\n","- **Original distribution**: Two peaks appear because the sine wave spends more time near its extrema (± amplitude). This is expected and not “true bimodality.”  \n","- **Min–Max scaling**: Same shape, but mapped into `[0, 1]`.  \n","- **Standardization**: Same shape, but shifted and rescaled to mean 0, variance 1.  \n","- **Power Transform**: Looks almost identical to Standardization here, because the signal is already symmetric (no skewness to correct).  \n","\n","**Important:** Power Transform mainly helps when data is **skewed with long tails**. For symmetric oscillatory signals like these, it provides little benefit over standardization.  \n","\n","\n","**🔹 Why this matters**\n","- Distance-based ML algorithms (**SVMs, kNN, neural networks**) need scaled inputs.  \n","- Algorithms assuming Gaussian features (**PCA, linear regression**) may benefit from transformations if skewness exists.  \n","- For sinusoidal signals, Standardization is usually sufficient.  \n"],"metadata":{"id":"jprvx72RHyFH"}},{"cell_type":"code","source":["# ---- Feature Extraction ----\n","\n","from scipy.stats import kurtosis, skew\n","\n","def extract_features(sig, fs, band=(1000, 5000)):\n","    \"\"\"\n","    Extract time-domain and frequency-domain features from a 1D signal.\n","\n","    Parameters:\n","        sig (array-like): Input signal\n","        fs (float): Sampling frequency (Hz)\n","        band (tuple): Frequency range for band power calculation (Hz)\n","\n","    Returns:\n","        features (dict): Dictionary of extracted features\n","    \"\"\"\n","    sig = np.ravel(sig)  # ensure 1D\n","    if len(sig) == 0:\n","        # handle empty signal gracefully\n","        return {key: np.nan for key in [\"mean\",\"std\",\"rms\",\"peak2peak\",\n","                                        \"skewness\",\"kurtosis\",\"dominant_freq\",\"band_power\"]}\n","\n","    # Time-domain features\n","    features = {\n","        \"mean\": np.mean(sig),\n","        \"std\": np.std(sig),\n","        \"rms\": np.sqrt(np.mean(sig**2)),\n","        \"peak2peak\": np.ptp(sig),\n","        \"skewness\": skew(sig),\n","        \"kurtosis\": kurtosis(sig),\n","    }\n","\n","    # Frequency-domain features\n","    yf = np.abs(fft(sig))[:len(sig)//2]      # magnitude spectrum\n","    xf = fftfreq(len(sig), 1/fs)[:len(sig)//2]  # corresponding frequencies\n","\n","    features[\"dominant_freq\"] = xf[np.argmax(yf)]\n","    features[\"band_power\"] = np.sum(yf[(xf > band[0]) & (xf < band[1])])\n","\n","    return features\n","\n","# Example usage\n","feat = extract_features(signal_series.fillna(method=\"ffill\"), fs)\n","display(\"Extracted Features:\", feat)"],"metadata":{"id":"DcjZEyHwzZ_p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 🛠 Feature Extraction\n","\n","### 🔹 What we are doing\n","- Extracting **numerical descriptors** from signals that summarize their characteristics.\n","- Two main types of features:\n","  1. **Time-domain features:** mean, std, RMS, peak-to-peak, skewness, kurtosis.\n","  2. **Frequency-domain features:** dominant frequency, band power.\n","\n","---\n","\n","### 🔹 Why it matters\n","- Raw signals are often high-dimensional and redundant; features reduce dimensionality while keeping essential information.\n","- Time-domain features capture **overall amplitude, variability, and shape**.\n","- Frequency-domain features capture **periodicities and energy in specific frequency bands**.\n","\n","---\n","\n","### 🔹 Interpretation\n","- **Mean:** average signal amplitude.\n","- **Std:** variability of the signal.\n","- **RMS:** overall energy magnitude.\n","- **Peak-to-peak:** amplitude range.\n","- **Skewness:** asymmetry of signal distribution.\n","- **Kurtosis:** presence of heavy tails or sharp peaks.\n","- **Dominant frequency:** main periodic component.\n","- **Band power:** energy in a specific frequency range."],"metadata":{"id":"Z7LpwcnYSzWn"}},{"cell_type":"code","source":["# ---- Windowed Feature Extraction (Non-overlapping) ----\n","\n","window_length = 5000  # number of samples per frame\n","feature_matrix = []\n","labels = []\n","\n","for i, mat_file in enumerate(mat_files[:20]):  # demo subset\n","    data = sio.loadmat(mat_file, squeeze_me=True, struct_as_record=False)\n","    measurement_key = [k for k in data.keys() if not k.startswith('__')][0]\n","    measurement = data[measurement_key]\n","\n","    # Extract raw signal data\n","    sig = measurement.Y.flat[0] if isinstance(measurement.Y, np.ndarray) else measurement.Y\n","    sig_data = sig.Data if hasattr(sig, \"Data\") else np.array(sig)\n","    fs = sig.Raster if hasattr(sig, \"Raster\") and isinstance(sig.Raster, (int,float)) else 64000\n","\n","    # Slide through the signal in non-overlapping windows\n","    num_frames = len(sig_data) // window_length\n","    for j in range(num_frames):\n","        frame = sig_data[j*window_length:(j+1)*window_length]\n","        feat = extract_features(frame, fs)\n","        feature_matrix.append(list(feat.values()))\n","        labels.append(measurement.Label if hasattr(measurement, \"Label\") else \"Unknown\")\n","\n","feature_matrix = np.array(feature_matrix)\n","labels = np.array(labels)\n","\n","print(f'Feature Matrix Size: {feature_matrix.shape}\\nLabels Size: {labels.shape}')"],"metadata":{"id":"m7lAJ-Dfzdk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train / Test Split\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    feature_matrix, labels, test_size=0.3, random_state=42, stratify=labels\n",")\n","\n","print(\"Train / Test Split:\")\n","print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)"],"metadata":{"id":"pMF4pJ6ja27L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train / Validation / Test Split\n","\n","# First split into train+val and test\n","X_temp, X_test, y_temp, y_test = train_test_split(\n","    feature_matrix, labels, test_size=0.2, stratify=labels, random_state=42\n",")\n","\n","# Then split train into train and validation\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42\n",")  # 0.25 x 0.8 = 0.2\n","print(\"Train / Validation / Test Split:\")\n","print(\"Train size:\", X_train.shape, \" Validation size:\", X_val.shape, \" Test size:\", X_test.shape)"],"metadata":{"id":"q8v2WrFua5Le"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ---- K-Fold Cross-Validation ----\n","from sklearn.model_selection import KFold, cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","clf = RandomForestClassifier()\n","scores = cross_val_score(clf, feature_matrix, labels, cv=kf)\n","\n","print(\"Cross-validation scores:\", scores)\n","print(\"Mean CV score:\", scores.mean())"],"metadata":{"id":"tditY_XecExS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📊 Data Splitting & Validation\n","\n","- Before training ML models, we must **evaluate performance reliably**.  \n","- The goal: ensure the model generalizes, not just memorizes training data.  \n","\n","---\n","\n","### 🔹 Hold-out Split  \n","- Split dataset once into **train** and **test** sets.  \n","- Simple and fast, but performance may depend on which samples land in the test set.  \n","\n","---\n","\n","### 🔹 Train / Validation / Test  \n","- Often, we need a **validation set** to tune hyperparameters.  \n","- Typical ratios: 60% train / 20% validation / 20% test.  \n","- Test set is kept aside until the very end → avoids overfitting to validation.  \n","\n","---\n","\n","### 🔹 K-Fold Cross-Validation  \n","- Data is split into **k folds** (e.g., k=5).  \n","- Each fold is used once as test, the rest as train.  \n","- More robust than a single hold-out split, especially with small datasets.  \n","- Downside: more computation (train the model k times).  \n","\n","---\n","\n","⚠️ **Note:**  \n","- In industrial signal processing, datasets may be **limited** (few fault cases).  \n","- Cross-validation helps use data more effectively, while still testing generalization.  "],"metadata":{"id":"Sdvw2CKtcjTd"}},{"cell_type":"markdown","source":["## 📝 Exercises — Chapter 3: Data Preprocessing\n","\n","*Hint: use fixed random seeds (e.g. 42) for any simulated noise/missing-data steps so results are reproducible for grading.*\n","\n","These exercises focus on **signal cleaning, transformation, and preparation for ML**. Apply them on the Paderborn dataset and optionally on your own dataset.\n","\n","1. **Handling Missing Values**\n","   - Simulate missing samples in a signal (e.g., remove ~1% for realism, or 5–10% to stress-test methods, using a fixed random seed).\n","   - Try different fill methods:\n","     - `ffill`, `bfill` (forward/backward fill in pandas).\n","     - Interpolation: `linear`, `polynomial`, `spline`, `time`.\n","     - Mean/median filling.\n","   - Compare the **frequency spectra (FFT/PSD)** of the original vs repaired signals.\n","   - Also inspect a **zoomed-in time window** — some spectral changes are subtle.\n","\n","2. **Outlier Detection & Correction**\n","   - Detect outliers with both:\n","     - **IQR method** (points below Q1 − 1.5·IQR or above Q3 + 1.5·IQR).\n","     - **Z-score method**.\n","   - Show a **small table**: counts of flagged points by each method, and their overlap.\n","   - Correct outliers:\n","     - Use **Winsorization/capping** (`np.clip` or `scipy.stats.mstats.winsorize`) for plausible extremes.\n","     - Remove only values that are clearly impossible or known-bad.\n","     - Or impute with median/mean/predicted values.\n","   - Plot **before/after** and zoom in on corrected regions to ensure you didn’t erase real fault signatures.\n","   - **Optional:** Define domain-specific thresholds, if possible.\n","\n","3. **Noise Filtering**\n","   - Add **synthetic noise** (impulse or Gaussian) to a clean signal.\n","   - Apply **median filter** (kernel sizes 3, 5, 7) and **moving average filter**.\n","   - Plot **original, noisy, and cleaned signals**.\n","   - Compare results and discuss which method removes noise best while preserving sharp transitions.\n","\n","4. **Scaling & Transformations**\n","   - Add a log transform (np.log1p) and observe its effect on the signal histogram. If values are non-positive, shift the data slightly to make them positive before applying log1p. Compare before/after distributions.\n","\n","   - **Optional:** Apply the same transformations to a **positively skewed dataset** (e.g., energy consumption values) and compare effects.\n","   - Reflect: When does PowerTransform provide meaningful changes? When does it behave like Standardization?\n","\n","5. **Feature Extraction**\n","   - Extract features from a signal of a different class and compare features of two different classes (e.g., healthy vs faulty signals).\n","   - Extend the **Automation Challenge** from Chapter 2 to include **frequency-domain features** (dominant frequency, band power) in the feature matrix.  \n","   - Repeat the same visualization and exploration.  \n","   - If signals are long, extract features on **non-overlapping frames/windows**.\n","\n","6. **Data Split**\n","   - **Train/Validation/Test Split:** Modify the existing hold-out code to create three subsets: 70% for training, 15% for validation, and 15% for testing. Print the shapes of each subset to confirm the split.\n","\n","   - **Problem to solve (label mapping + encoding):** In the Paderborn dataset, `measurement.Label` does not exist. Students should:\n","      1. Assign the correct **class labels** by mapping folder names (e.g., `K001`, `K002`, …) to the dataset’s test-case/classes using the official Paderborn mapping (paper/website). Store the mapping in a `dict` or small CSV/JSON file.\n","      2. Apply **label encoding** to prepare labels for ML:\n","         - Use `sklearn.preprocessing.LabelEncoder` (or `pd.Categorical`) to convert class names → integer labels.\n","         - If needed for model input, use `OneHotEncoder` (or `pd.get_dummies`) to produce one-hot vectors.\n","      3. Verify correctness:\n","         - Print a few `(mat_file, inferred_label, encoded_label)` examples.\n","         - Print class counts: `np.unique(encoded_labels, return_counts=True)` and check for unexpected classes or imbalances.\n","      4. Use the (encoded) labels in your train/validation/test split.\n","\n","   - **Challenge — Random Seeds:** Run your train/validation/test split multiple times with different `random_state` values and compare how the indices of samples in each subset change.\n","\n","   - **K-Fold Cross-Validation Check:** After solving the label mapping and encoding problem, repeat the 5-fold (or k-fold) cross-validation from Chapter 3 and compare the results with your original cross-validation before label mapping. Discuss any differences in scores and why encoding affects the process.\n","\n","7. **Project Dataset Processing**\n","   - Apply all necessary preprocessing on your own industrial dataset.\n","   - Make sure it is **ready for feature extraction and ML in the next chapter**."],"metadata":{"id":"qjbOLmCPEJC1"}},{"cell_type":"markdown","source":["# 📘 Chapter 4: Classical Machine Learning Methods\n","---\n","We now apply ML models (e.g., SVM, Random Forest) to extracted features."],"metadata":{"id":"REH_DF9Vyj4b"}},{"cell_type":"markdown","source":["# 📘 Chapter 5: Deep Learning Architectures\n","---\n","We extend to deep learning (CNNs, RNNs, Transformers) using raw signals."],"metadata":{"id":"vtGADBeSymWn"}}]}